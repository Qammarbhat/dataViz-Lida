{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fc15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55e30d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongodb+srv://chinarquantum:GHboz7Yq0DX1yVtF@cqai.wvbbg.mongodb.net/?retryWrites=true&w=majority&appName=CQAI\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# os.environ.clear()\n",
    "load_dotenv()\n",
    "\n",
    "# MongoDB connection URI (change if needed)\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\")\n",
    "print(MONGO_URI)\n",
    "\n",
    "DB_NAME = \"ems\"\n",
    "ATTENDANCE_COLLECTION = \"attendances\"\n",
    "USER_COLLECTION = \"users\"\n",
    "OUTPUT_CSV = \"attendances_export.csv\"\n",
    "\n",
    "def export_merged_data_to_csv():\n",
    "    client = pymongo.MongoClient(MONGO_URI)\n",
    "    db = client[DB_NAME]\n",
    "\n",
    "    # Fetch documents\n",
    "    attendances = list(db[ATTENDANCE_COLLECTION].find())\n",
    "    users = list(db[USER_COLLECTION].find())\n",
    "\n",
    "    if not attendances or not users:\n",
    "        print(\"One or both collections are empty.\")\n",
    "        return\n",
    "\n",
    "    # Create DataFrames\n",
    "    attend_df = pd.DataFrame(attendances)\n",
    "    users_df = pd.DataFrame(users)\n",
    "\n",
    "    # Convert ObjectId to string for merging\n",
    "    attend_df['user'] = attend_df['user'].astype(str)\n",
    "    users_df['_id'] = users_df['_id'].astype(str)\n",
    "\n",
    "    # Select only necessary user fields\n",
    "    user_fields = ['_id', 'name', 'position', 'joiningDate', 'linkedInId', 'githubId', 'leaveDate', 'address']\n",
    "    users_df = users_df[user_fields]\n",
    "\n",
    "    # Merge: attendances.user -> users._id\n",
    "    merged_df = pd.merge(attend_df, users_df, left_on='user', right_on='_id', how='left')\n",
    "    # print(merged_df.head())\n",
    "    # Export to CSV\n",
    "    # merged_df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"Exported merged data to '{OUTPUT_CSV}' with {len(merged_df)} records.\")\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87e19f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported merged data to 'attendances_export.csv' with 111 records.\n"
     ]
    }
   ],
   "source": [
    "merged_df = export_merged_data_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "112cd6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id_x', 'user', 'date', 'checkIn', 'totalHours', 'status', 'breaks',\n",
       "       'createdAt', 'updatedAt', '__v', 'checkOut', 'overtimeHours',\n",
       "       'regularHours', 'overworkingMinutes', '_id_y', 'name', 'position',\n",
       "       'joiningDate', 'linkedInId', 'githubId', 'leaveDate', 'address'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b716611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop([\"_id_x\", \"__v\", \"_id_y\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "332b265d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>date</th>\n",
       "      <th>checkIn</th>\n",
       "      <th>totalHours</th>\n",
       "      <th>status</th>\n",
       "      <th>breaks</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>checkOut</th>\n",
       "      <th>overtimeHours</th>\n",
       "      <th>regularHours</th>\n",
       "      <th>overworkingMinutes</th>\n",
       "      <th>name</th>\n",
       "      <th>position</th>\n",
       "      <th>joiningDate</th>\n",
       "      <th>linkedInId</th>\n",
       "      <th>githubId</th>\n",
       "      <th>leaveDate</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67175cb20d39c16d5f2af408</td>\n",
       "      <td>2025-01-08 03:58:13.257</td>\n",
       "      <td>{'time': 2025-01-08 03:58:13.260000, 'location...</td>\n",
       "      <td>14.03</td>\n",
       "      <td>present</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025-01-08 03:58:13.272</td>\n",
       "      <td>2025-01-16 04:06:54.422</td>\n",
       "      <td>{'time': 2025-01-08 18:00:00, 'location': {'la...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67175cb20d39c16d5f2af408</td>\n",
       "      <td>2025-01-10 06:06:30.675</td>\n",
       "      <td>{'time': 2025-01-10 06:06:30.676000, 'location...</td>\n",
       "      <td>11.89</td>\n",
       "      <td>present</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025-01-10 06:06:30.677</td>\n",
       "      <td>2025-01-16 04:06:55.074</td>\n",
       "      <td>{'time': 2025-01-10 18:00:00, 'location': {'la...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67175cb20d39c16d5f2af408</td>\n",
       "      <td>2025-01-13 07:23:17.888</td>\n",
       "      <td>{'time': 2025-01-13 07:23:17.889000, 'location...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>present</td>\n",
       "      <td>[{'startTime': 2025-01-13 07:24:27.239000, 'en...</td>\n",
       "      <td>2025-01-13 07:23:17.890</td>\n",
       "      <td>2025-01-13 07:25:02.079</td>\n",
       "      <td>{'time': 2025-01-13 07:25:02.077000, 'location...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6784c241e12fe0ae8dd55e46</td>\n",
       "      <td>2025-01-13 07:50:54.377</td>\n",
       "      <td>{'time': 2025-01-13 07:50:54.377000, 'location...</td>\n",
       "      <td>10.15</td>\n",
       "      <td>present</td>\n",
       "      <td>[{'startTime': 2025-01-13 07:51:27.071000, 'en...</td>\n",
       "      <td>2025-01-13 07:50:54.378</td>\n",
       "      <td>2025-01-16 04:06:55.965</td>\n",
       "      <td>{'time': 2025-01-13 18:00:00, 'location': {'la...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sahreen Haider</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>sahreen-haider</td>\n",
       "      <td>sahreen-haider</td>\n",
       "      <td>[]</td>\n",
       "      <td>Pulwama, Jammu and Kashmir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6784cdfce12fe0ae8dd56107</td>\n",
       "      <td>2025-01-14 06:16:21.661</td>\n",
       "      <td>{'time': 2025-01-14 06:16:21.662000, 'location...</td>\n",
       "      <td>11.73</td>\n",
       "      <td>present</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025-01-14 06:16:21.663</td>\n",
       "      <td>2025-01-16 04:06:56.405</td>\n",
       "      <td>{'time': 2025-01-14 18:00:00, 'location': {'la...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Owais Bin Mushtaq</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>owais-bin-mushtaq-066a04211</td>\n",
       "      <td>HakimOwais</td>\n",
       "      <td>[{'startDate': '2025-01-15', 'leaveDate': '202...</td>\n",
       "      <td>Pulwama, J&amp;K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user                    date  \\\n",
       "0  67175cb20d39c16d5f2af408 2025-01-08 03:58:13.257   \n",
       "1  67175cb20d39c16d5f2af408 2025-01-10 06:06:30.675   \n",
       "2  67175cb20d39c16d5f2af408 2025-01-13 07:23:17.888   \n",
       "3  6784c241e12fe0ae8dd55e46 2025-01-13 07:50:54.377   \n",
       "4  6784cdfce12fe0ae8dd56107 2025-01-14 06:16:21.661   \n",
       "\n",
       "                                             checkIn  totalHours   status  \\\n",
       "0  {'time': 2025-01-08 03:58:13.260000, 'location...       14.03  present   \n",
       "1  {'time': 2025-01-10 06:06:30.676000, 'location...       11.89  present   \n",
       "2  {'time': 2025-01-13 07:23:17.889000, 'location...        0.03  present   \n",
       "3  {'time': 2025-01-13 07:50:54.377000, 'location...       10.15  present   \n",
       "4  {'time': 2025-01-14 06:16:21.662000, 'location...       11.73  present   \n",
       "\n",
       "                                              breaks               createdAt  \\\n",
       "0                                                 [] 2025-01-08 03:58:13.272   \n",
       "1                                                 [] 2025-01-10 06:06:30.677   \n",
       "2  [{'startTime': 2025-01-13 07:24:27.239000, 'en... 2025-01-13 07:23:17.890   \n",
       "3  [{'startTime': 2025-01-13 07:51:27.071000, 'en... 2025-01-13 07:50:54.378   \n",
       "4                                                 [] 2025-01-14 06:16:21.663   \n",
       "\n",
       "                updatedAt                                           checkOut  \\\n",
       "0 2025-01-16 04:06:54.422  {'time': 2025-01-08 18:00:00, 'location': {'la...   \n",
       "1 2025-01-16 04:06:55.074  {'time': 2025-01-10 18:00:00, 'location': {'la...   \n",
       "2 2025-01-13 07:25:02.079  {'time': 2025-01-13 07:25:02.077000, 'location...   \n",
       "3 2025-01-16 04:06:55.965  {'time': 2025-01-13 18:00:00, 'location': {'la...   \n",
       "4 2025-01-16 04:06:56.405  {'time': 2025-01-14 18:00:00, 'location': {'la...   \n",
       "\n",
       "   overtimeHours  regularHours  overworkingMinutes               name  \\\n",
       "0            NaN           NaN                 NaN                NaN   \n",
       "1            NaN           NaN                 NaN                NaN   \n",
       "2            NaN           NaN                 NaN                NaN   \n",
       "3            NaN           NaN                 NaN     Sahreen Haider   \n",
       "4            NaN           NaN                 NaN  Owais Bin Mushtaq   \n",
       "\n",
       "                    position joiningDate                   linkedInId  \\\n",
       "0                        NaN         NaT                          NaN   \n",
       "1                        NaN         NaT                          NaN   \n",
       "2                        NaN         NaT                          NaN   \n",
       "3  Machine Learning Engineer  2024-03-01               sahreen-haider   \n",
       "4             Data Scientist  2023-11-01  owais-bin-mushtaq-066a04211   \n",
       "\n",
       "         githubId                                          leaveDate  \\\n",
       "0             NaN                                                NaN   \n",
       "1             NaN                                                NaN   \n",
       "2             NaN                                                NaN   \n",
       "3  sahreen-haider                                                 []   \n",
       "4      HakimOwais  [{'startDate': '2025-01-15', 'leaveDate': '202...   \n",
       "\n",
       "                      address  \n",
       "0                         NaN  \n",
       "1                         NaN  \n",
       "2                         NaN  \n",
       "3  Pulwama, Jammu and Kashmir  \n",
       "4                Pulwama, J&K  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72924d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(OUTPUT_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0196de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample Document from 'attendances' Collection ---\n",
      "- _id: ObjectId\n",
      "- user: ObjectId\n",
      "- date: datetime\n",
      "- checkIn: dict\n",
      "  - time: datetime\n",
      "  - location: dict\n",
      "    - latitude: float\n",
      "    - longitude: float\n",
      "- totalHours: float\n",
      "- status: str\n",
      "- breaks: list (contains:\n",
      "    - Dictionary:\n",
      "    - startTime: datetime\n",
      "    - endTime: datetime\n",
      "    - duration: float\n",
      "    - _id: ObjectId\n",
      "- createdAt: datetime\n",
      "- updatedAt: datetime\n",
      "- __v: int\n",
      "- checkOut: dict\n",
      "  - time: datetime\n",
      "  - location: dict\n",
      "    - latitude: float\n",
      "    - longitude: float\n",
      "\n",
      "--- Sample Document from 'users' Collection ---\n",
      "- _id: ObjectId\n",
      "- email: str\n",
      "- password: str\n",
      "- joiningDate: datetime\n",
      "- position: str\n",
      "- name: str\n",
      "- aadhar: str\n",
      "- panNo: str\n",
      "- isSuperUser: bool\n",
      "- isApproved: str\n",
      "- invitedBy: ObjectId\n",
      "- image: str\n",
      "- address: str\n",
      "- linkedInId: str\n",
      "- phone: str\n",
      "- githubId: str\n",
      "- dateOfBirth: datetime\n",
      "- gender: str\n",
      "- leaveDate: list (contains:\n",
      "    - Dictionary:\n",
      "    - startDate: str\n",
      "    - leaveDate: str\n",
      "    - leave_status: str\n",
      "    - leaveDays: int\n",
      "    - reason: str\n",
      "    - description: str\n",
      "    - assignedAdmin: ObjectId\n",
      "    - _id: ObjectId\n",
      "- __v: int\n",
      "- approvalDate: datetime\n",
      "- tags: list (contains:\n",
      "    - str)\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# os.environ.clear()\n",
    "load_dotenv()\n",
    "\n",
    "# MongoDB connection URI (change if needed)\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\")\n",
    "DATABASE_NAME = \"ems\"  # Replace with your database name\n",
    "\n",
    "try:\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[DATABASE_NAME]\n",
    "    attendances_collection = db[\"attendances\"]\n",
    "    users_collection = db[\"users\"]\n",
    "\n",
    "    # Fetch one document from each collection\n",
    "    sample_attendance = attendances_collection.find_one()\n",
    "    sample_user = users_collection.find_one()\n",
    "\n",
    "    print(\"--- Sample Document from 'attendances' Collection ---\")\n",
    "    if sample_attendance:\n",
    "        def print_structure(document, indent=0):\n",
    "            for key, value in document.items():\n",
    "                print(\"  \" * indent + f\"- {key}: {type(value).__name__}\", end=\"\")\n",
    "                if isinstance(value, dict):\n",
    "                    print()\n",
    "                    print_structure(value, indent + 1)\n",
    "                elif isinstance(value, list):\n",
    "                    print(\" (contains:\")\n",
    "                    if value:\n",
    "                        if isinstance(value[0], dict):\n",
    "                            print(\"  \" * (indent + 1) + \"  - Dictionary:\")\n",
    "                            print_structure(value[0], indent + 2)\n",
    "                        else:\n",
    "                            print(\"  \" * (indent + 1) + f\"  - {type(value[0]).__name__})\")\n",
    "                    else:\n",
    "                        print(\")\")\n",
    "                else:\n",
    "                    print()\n",
    "        print_structure(sample_attendance)\n",
    "    else:\n",
    "        print(\"No documents found in the 'attendances' collection.\")\n",
    "\n",
    "    print(\"\\n--- Sample Document from 'users' Collection ---\")\n",
    "    if sample_user:\n",
    "        print_structure(sample_user)\n",
    "    else:\n",
    "        print(\"No documents found in the 'users' collection.\")\n",
    "\n",
    "except ConnectionError as e:\n",
    "    print(f\"Could not connect to MongoDB: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if 'client' in locals():\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8450b253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'employee_dataset.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# os.environ.clear()\n",
    "load_dotenv()\n",
    "\n",
    "# MongoDB connection URI (change if needed)\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\")\n",
    "DATABASE_NAME = \"ems\"  # Replace with your database name\n",
    "\n",
    "try:\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[DATABASE_NAME]\n",
    "    attendances_collection = db[\"attendances\"]\n",
    "    users_collection = db[\"users\"]\n",
    "\n",
    "    # Fetch all users\n",
    "    users_data = list(users_collection.find())\n",
    "\n",
    "    # Prepare a list to store employee data for the CSV\n",
    "    employee_data = []\n",
    "\n",
    "    for user in users_data:\n",
    "        user_id = str(user['_id'])\n",
    "        attendances = list(attendances_collection.find({\"user\": user['_id']}))\n",
    "\n",
    "        total_working_hours = 0\n",
    "        check_in_check_out_count = 0\n",
    "        working_days = set()\n",
    "        leave_dates_list = []\n",
    "        total_breaks = 0\n",
    "        total_break_duration = 0\n",
    "        break_start_end_times = []\n",
    "\n",
    "        if attendances:\n",
    "            check_in_check_out_count = len(attendances)\n",
    "            for attendance in attendances:\n",
    "                if attendance.get('totalHours'):\n",
    "                    total_working_hours += attendance['totalHours']\n",
    "                if attendance.get('checkIn') and attendance.get('checkOut'):\n",
    "                    working_days.add(attendance['date'].strftime('%Y-%m-%d'))\n",
    "                if attendance.get('breaks'):\n",
    "                    total_breaks += len(attendance['breaks'])\n",
    "                    for break_entry in attendance['breaks']:\n",
    "                        if break_entry.get('duration'):\n",
    "                            total_break_duration += break_entry['duration']\n",
    "                        if break_entry.get('startTime') and break_entry.get('endTime'):\n",
    "                            break_start_end_times.append(\n",
    "                                f\"{break_entry['startTime'].strftime('%Y-%m-%d %H:%M:%S')} - {break_entry['endTime'].strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "                            )\n",
    "\n",
    "        leave_count = len(user.get('leaveDate', []))\n",
    "        leave_dates_raw = user.get('leaveDate', [])\n",
    "        for leave in leave_dates_raw:\n",
    "            leave_dates_list.append(f\"{leave.get('startDate', 'N/A')} to {leave.get('leaveDate', 'N/A')}\")\n",
    "\n",
    "        employee_info = {\n",
    "            \"Employee ID\": user_id,\n",
    "            \"Employee Name\": user.get('name', 'N/A'),\n",
    "            \"Employee Joining Date\": user.get('joiningDate', None).strftime('%Y-%m-%d') if user.get('joiningDate') else 'N/A',\n",
    "            \"Employee Gender\": user.get('gender', 'N/A'),\n",
    "            \"Employee Leave Count\": leave_count,\n",
    "            \"Total Working Hours Recorded\": total_working_hours,\n",
    "            \"Total Check-in Check-out Records\": check_in_check_out_count,\n",
    "            \"Working Days Count\": len(working_days),\n",
    "            \"Employee Status\": user.get('isApproved', 'N/A'),\n",
    "            \"Leave Dates\": \", \".join(leave_dates_list) if leave_dates_list else 'N/A',\n",
    "            \"Total Break Count\": total_breaks,\n",
    "            \"Total Break Duration (Hours)\": total_break_duration,\n",
    "            \"Break Start - End Times\": \"; \".join(break_start_end_times) if break_start_end_times else 'N/A',\n",
    "            \"Leave Details\": \", \".join([f\"From {leave.get('startDate', 'N/A')} to {leave.get('leaveDate', 'N/A')}\" for leave in leave_dates_raw]) if leave_dates_raw else 'N/A'\n",
    "        }\n",
    "        employee_data.append(employee_info)\n",
    "\n",
    "    # Create a Pandas DataFrame\n",
    "    df = pd.DataFrame(employee_data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_file_path = \"employee_dataset.csv\"\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"CSV file '{csv_file_path}' created successfully.\")\n",
    "\n",
    "except ConnectionError as e:\n",
    "    print(f\"Could not connect to MongoDB: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if 'client' in locals():\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac214c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'NoneType' object has no attribute 'strftime'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# os.environ.clear()\n",
    "load_dotenv()\n",
    "\n",
    "# MongoDB connection URI (change if needed)\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\")\n",
    "DATABASE_NAME = \"ems\"  # Replace with your database name\n",
    "\n",
    "try:\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[DATABASE_NAME]\n",
    "    attendances_collection = db[\"attendances\"]\n",
    "    users_collection = db[\"users\"]\n",
    "\n",
    "    # Fetch all attendance records\n",
    "    attendances_data = list(attendances_collection.find())\n",
    "\n",
    "    # Prepare a list to store attendance data for the CSV\n",
    "    attendance_data = []\n",
    "\n",
    "    for attendance in attendances_data:\n",
    "        user_id = attendance['user']\n",
    "        user = users_collection.find_one({\"_id\": user_id})  # Find the user for this attendance record\n",
    "\n",
    "        if user:\n",
    "            user_name = user.get('name', 'N/A')\n",
    "            user_joining_date = user.get('joiningDate', None).strftime('%Y-%m-%d') if user and user.get('joiningDate') else 'N/A'\n",
    "            user_gender = user.get('gender', 'N/A')\n",
    "            leave_count = len(user.get('leaveDate', []))\n",
    "            leave_dates_raw = user.get('leaveDate', [])\n",
    "            leave_dates_list = [f\"From {leave.get('startDate', 'N/A')} to {leave.get('leaveDate', 'N/A')}\" for leave in leave_dates_raw] if leave_dates_raw else ['N/A']\n",
    "            employee_status = user.get('isApproved', 'N/A')\n",
    "        else:\n",
    "            user_name = 'N/A'\n",
    "            user_joining_date = 'N/A'\n",
    "            user_gender = 'N/A'\n",
    "            leave_count = 0\n",
    "            leave_dates_list = ['N/A']\n",
    "            employee_status = 'N/A'\n",
    "\n",
    "        check_in_check_out_count = 1  # Each attendance record represents one check-in/check-out\n",
    "        working_days = {attendance['date'].strftime('%Y-%m-%d')}  # Use a set to store unique dates\n",
    "        total_breaks = len(attendance.get('breaks', []))\n",
    "        total_break_duration = sum(b.get('duration', 0) for b in attendance.get('breaks', []))\n",
    "        break_start_end_times = [\n",
    "            f\"{b.get('startTime', datetime(1970,1,1)).strftime('%Y-%m-%d %H:%M:%S')} - {b.get('endTime', datetime(1970,1,1)).strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "            for b in attendance.get('breaks', [])\n",
    "        ]\n",
    "\n",
    "        attendance_info = {\n",
    "            \"Employee ID\": str(user_id),\n",
    "            \"Employee Name\": user_name,\n",
    "            \"Employee Joining Date\": user_joining_date,\n",
    "            \"Employee Gender\": user_gender,\n",
    "            \"Employee Leave Count\": leave_count,\n",
    "            \"Total Working Hours Recorded\": attendance.get('totalHours', 0),\n",
    "            \"Total Check-in Check-out Records\": check_in_check_out_count,\n",
    "            \"Working Days Count\": len(working_days),\n",
    "            \"Employee Status\": employee_status,\n",
    "            \"Leave Dates\": \", \".join(leave_dates_list),\n",
    "            \"Total Break Count\": total_breaks,\n",
    "            \"Total Break Duration (Hours)\": total_break_duration,\n",
    "            \"Break Start - End Times\": \"; \".join(break_start_end_times) if break_start_end_times else 'N/A',\n",
    "            \"Leave Details\": \", \".join(leave_dates_list)\n",
    "        }\n",
    "        attendance_data.append(attendance_info)\n",
    "\n",
    "    # Create a Pandas DataFrame\n",
    "    df = pd.DataFrame(attendance_data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_file_path = \"attendance_dataset.csv\"\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"CSV file '{csv_file_path}' created successfully.\")\n",
    "\n",
    "except ConnectionError as e:\n",
    "    print(f\"Could not connect to MongoDB: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if 'client' in locals():\n",
    "        client.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b41b1d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'attendance_dataset.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# os.environ.clear()\n",
    "load_dotenv()\n",
    "\n",
    "# MongoDB connection URI (change if needed)\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\")\n",
    "DATABASE_NAME = \"ems\"  # Replace with your database name\n",
    "\n",
    "try:\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[DATABASE_NAME]\n",
    "    attendances_collection = db[\"attendances\"]\n",
    "    users_collection = db[\"users\"]\n",
    "\n",
    "    # Fetch all attendance records\n",
    "    attendances_data = list(attendances_collection.find())\n",
    "\n",
    "    # Prepare a list to store attendance data for the CSV\n",
    "    attendance_data = []\n",
    "\n",
    "    for attendance in attendances_data:\n",
    "        user_id = attendance['user']\n",
    "        user = users_collection.find_one({\"_id\": user_id})  # Find the user for this attendance record\n",
    "\n",
    "        if user:\n",
    "            user_name = user.get('name', 'N/A')\n",
    "            user_joining_date = user.get('joiningDate', None)\n",
    "            user_joining_date_str = user_joining_date.strftime('%Y-%m-%d') if isinstance(user_joining_date, datetime) else 'N/A'\n",
    "            user_gender = user.get('gender', 'N/A')\n",
    "            leave_count = len(user.get('leaveDate', []))\n",
    "            leave_dates_raw = user.get('leaveDate', [])\n",
    "            leave_dates_list = [f\"From {leave.get('startDate', 'N/A')} to {leave.get('leaveDate', 'N/A')}\" for leave in leave_dates_raw] if leave_dates_raw else ['N/A']\n",
    "            employee_status = user.get('isApproved', 'N/A')\n",
    "        else:\n",
    "            user_name = 'N/A'\n",
    "            user_joining_date_str = 'N/A'\n",
    "            user_gender = 'N/A'\n",
    "            leave_count = 0\n",
    "            leave_dates_list = ['N/A']\n",
    "            employee_status = 'N/A'\n",
    "\n",
    "        check_in_check_out_count = 1  # Each attendance record represents one check-in/check-out\n",
    "        working_days = {attendance['date'].strftime('%Y-%m-%d')}  # Use a set to store unique dates\n",
    "        total_breaks = len(attendance.get('breaks', []))\n",
    "        total_break_duration = sum(b.get('duration', 0) for b in attendance.get('breaks', []))\n",
    "        break_start_end_times = []\n",
    "        for b in attendance.get('breaks', []):\n",
    "            start_time = b.get('startTime')\n",
    "            end_time = b.get('endTime')\n",
    "            start_time_str = start_time.strftime('%Y-%m-%d %H:%M:%S') if isinstance(start_time, datetime) else 'N/A'\n",
    "            end_time_str = end_time.strftime('%Y-%m-%d %H:%M:%S') if isinstance(end_time, datetime) else 'N/A'\n",
    "            break_start_end_times.append(f\"{start_time_str} - {end_time_str}\")\n",
    "\n",
    "        attendance_info = {\n",
    "            \"Employee ID\": str(user_id),\n",
    "            \"Employee Name\": user_name,\n",
    "            \"Employee Joining Date\": user_joining_date_str,\n",
    "            \"Employee Gender\": user_gender,\n",
    "            \"Employee Leave Count\": leave_count,\n",
    "            \"Total Working Hours Recorded\": attendance.get('totalHours', 0),\n",
    "            \"Total Check-in Check-out Records\": check_in_check_out_count,\n",
    "            \"Working Days Count\": len(working_days),\n",
    "            \"Employee Status\": employee_status,\n",
    "            \"Leave Dates\": \", \".join(leave_dates_list),\n",
    "            \"Total Break Count\": total_breaks,\n",
    "            \"Total Break Duration (Hours)\": total_break_duration,\n",
    "            \"Break Start - End Times\": \"; \".join(break_start_end_times) if break_start_end_times else 'N/A',\n",
    "            \"Leave Details\": \", \".join(leave_dates_list)\n",
    "        }\n",
    "        attendance_data.append(attendance_info)\n",
    "\n",
    "    # Create a Pandas DataFrame\n",
    "    df = pd.DataFrame(attendance_data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_file_path = \"attendance_dataset.csv\"\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"CSV file '{csv_file_path}' created successfully.\")\n",
    "\n",
    "except ConnectionError as e:\n",
    "    print(f\"Could not connect to MongoDB: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if 'client' in locals():\n",
    "        client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed94f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.10.0\n",
      "  Using cached matplotlib-3.10.0-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Using cached matplotlib-3.10.0-cp310-cp310-win_amd64.whl (8.0 MB)\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\admin\\.conda\\envs\\lida-demo\\lib\\site-packages)\n",
      "error: uninstall-no-record-file\n",
      "\n",
      "× Cannot uninstall matplotlib 3.10.0\n",
      "╰─> The package's contents are unknown: no RECORD file was found for matplotlib.\n",
      "\n",
      "hint: You might be able to recover from this via: pip install --force-reinstall --no-deps matplotlib==3.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall --no-deps matplotlib==3.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8607767f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.backends.registry'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbase64\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdisplay_charts_from_json\u001b[39m(json_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    Loads and displays base64-encoded charts from a JSON file.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m        json_path (str): Path to the JSON file containing chart data.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\.conda\\envs\\lida-demo\\lib\\site-packages\\matplotlib\\__init__.py:161\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrcsetup\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycler  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\.conda\\envs\\lida-demo\\lib\\site-packages\\matplotlib\\rcsetup.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackendFilter, backend_registry\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n",
      "File \u001b[1;32mc:\\Users\\admin\\.conda\\envs\\lida-demo\\lib\\site-packages\\matplotlib\\backends\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackendFilter, backend_registry  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# NOTE: plt.switch_backend() (called at import time) will add a \"backend\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# attribute here for backcompat.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m _QT_FORCE_QT5_BINDING \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib.backends.registry'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_charts_from_json(json_path: str = \"output.json\"):\n",
    "    \"\"\"\n",
    "    Loads and displays base64-encoded charts from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file containing chart data.\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        goal = item[\"goal\"]\n",
    "        base64_str = item[\"chart_base64\"]\n",
    "        img_bytes = base64.b64decode(base64_str)\n",
    "        image = Image.open(BytesIO(img_bytes))\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Goal {i+1}: {goal}\", fontsize=12)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8aedf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "\n",
    "# os.environ.clear()\n",
    "load_dotenv()\n",
    "\n",
    "# MongoDB connection URI (change if needed)\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\")\n",
    "DATABASE_NAME = \"ems\"  # Replace with your database name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406831fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV exported to 'raw_per_break_data.csv' with 82 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# MongoDB connection URI\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\")\n",
    "DATABASE_NAME = \"ems\"  # Your DB name\n",
    "\n",
    "def extract_raw_break_data():\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[DATABASE_NAME]\n",
    "\n",
    "    attendances = db['attendances']\n",
    "    users = db['users']\n",
    "\n",
    "    raw_break_data = []\n",
    "\n",
    "    # Create a mapping from user _id to name\n",
    "    user_map = {}\n",
    "    for user in users.find({}, {\"_id\": 1, \"name\": 1}):\n",
    "        user_map[str(user['_id'])] = user['name']\n",
    "\n",
    "    # Loop through attendance records\n",
    "    for record in attendances.find({}, {\n",
    "        \"user\": 1,\n",
    "        \"date\": 1,\n",
    "        \"breaks\": 1\n",
    "    }):\n",
    "        user_id = str(record.get(\"user\"))\n",
    "        employee_name = user_map.get(user_id, \"Unknown\")\n",
    "        attendance_date = record.get(\"date\")\n",
    "\n",
    "        for brk in record.get(\"breaks\", []):\n",
    "            break_start = brk.get(\"startTime\")\n",
    "            break_end = brk.get(\"endTime\")\n",
    "            duration = brk.get(\"duration\")\n",
    "\n",
    "            raw_break_data.append({\n",
    "                \"employee_id\": user_id,\n",
    "                \"employee_name\": employee_name,\n",
    "                \"date\": attendance_date.date() if attendance_date else None,\n",
    "                \"break_start_time\": break_start,\n",
    "                \"break_end_time\": break_end,\n",
    "                \"break_duration_hours\": duration\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(raw_break_data)\n",
    "\n",
    "    # Ensure datetime fields are in readable format\n",
    "    df['break_start_time'] = pd.to_datetime(df['break_start_time'], errors='coerce')\n",
    "    df['break_end_time'] = pd.to_datetime(df['break_end_time'], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_breaks = extract_raw_break_data()\n",
    "\n",
    "    # Save to CSV\n",
    "    output_path = \"raw_per_break_data.csv\"\n",
    "    df_breaks.to_csv(output_path, index=False)\n",
    "    print(f\"✅ CSV exported to '{output_path}' with {len(df_breaks)} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c6fbdc",
   "metadata": {},
   "source": [
    "# GENERATE CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6492151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported 200 rows to 'full_employee_attendance.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_1180\\3543968925.py:89: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df['checkin_time'] = pd.to_datetime(df['checkin_time'], errors='coerce')\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_1180\\3543968925.py:90: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df['checkout_time'] = pd.to_datetime(df['checkout_time'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\")\n",
    "DATABASE_NAME = \"ems\"\n",
    "\n",
    "def generate_full_attendance_csv():\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[DATABASE_NAME]\n",
    "    attendances = db['attendances']\n",
    "    users = db['users']\n",
    "\n",
    "    user_map = {}\n",
    "    for user in users.find():\n",
    "        user_id = str(user['_id'])\n",
    "        leaves = user.get(\"leaveDate\", [])\n",
    "        leave_lookup = set()\n",
    "        leave_type_map = {}\n",
    "\n",
    "        for leave in leaves:\n",
    "            try:\n",
    "                leave_dates = pd.date_range(leave[\"startDate\"], leave[\"leaveDate\"])\n",
    "                for d in leave_dates:\n",
    "                    leave_lookup.add(d.date())\n",
    "                    leave_type_map[d.date()] = leave.get(\"reason\", \"N/A\")\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        user_map[user_id] = {\n",
    "            \"employee_name\": user.get(\"name\"),\n",
    "            \"employee_gender\": user.get(\"gender\"),\n",
    "            \"employee_position\": user.get(\"position\"),\n",
    "            \"joining_date\": user.get(\"joiningDate\"),\n",
    "            \"leave_days\": leave_lookup,\n",
    "            \"leave_types\": leave_type_map\n",
    "        }\n",
    "\n",
    "    full_records = []\n",
    "\n",
    "    for record in attendances.find():\n",
    "        user_id = str(record.get(\"user\"))\n",
    "        user_info = user_map.get(user_id, {})\n",
    "        date = record.get(\"date\")\n",
    "        if not date:\n",
    "            continue\n",
    "\n",
    "        if isinstance(date, str):\n",
    "            date = pd.to_datetime(date, errors=\"coerce\")\n",
    "\n",
    "        if pd.isna(date):\n",
    "            continue\n",
    "\n",
    "        date_only = date.date()\n",
    "        weekday = date.strftime(\"%A\")\n",
    "        week_number = date.isocalendar()[1]\n",
    "\n",
    "        breaks = record.get(\"breaks\", [])\n",
    "        total_break_duration = sum([b.get(\"duration\", 0) for b in breaks])\n",
    "        break_count = len(breaks)\n",
    "        working_hours = record.get(\"totalHours\", 0)\n",
    "        overtime = max(0, working_hours - 8.0)\n",
    "\n",
    "        full_records.append({\n",
    "            \"employee_id\": user_id,\n",
    "            \"employee_name\": user_info.get(\"employee_name\", \"Unknown\"),\n",
    "            \"employee_gender\": user_info.get(\"employee_gender\", \"Unknown\"),\n",
    "            \"employee_position\": user_info.get(\"employee_position\", \"Unknown\"),\n",
    "            \"joining_date\": user_info.get(\"joining_date\"),\n",
    "            \"date\": date_only,\n",
    "            \"weekday\": weekday,\n",
    "            \"week_number\": week_number,\n",
    "            \"checkin_time\": record.get(\"checkIn\", {}).get(\"time\"),\n",
    "            \"checkout_time\": record.get(\"checkOut\", {}).get(\"time\"),\n",
    "            \"working_hours\": working_hours,\n",
    "            \"break_count\": break_count,\n",
    "            \"total_break_duration\": total_break_duration,\n",
    "            \"overtime_hours\": overtime,\n",
    "            \"is_leave_day\": date_only in user_info.get(\"leave_days\", set()),\n",
    "            \"leave_type\": user_info.get(\"leave_types\", {}).get(date_only, None)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(full_records)\n",
    "\n",
    "    # Convert times to readable formats\n",
    "    df['checkin_time'] = pd.to_datetime(df['checkin_time'], errors='coerce')\n",
    "    df['checkout_time'] = pd.to_datetime(df['checkout_time'], errors='coerce')\n",
    "    df['joining_date'] = pd.to_datetime(df['joining_date'], errors='coerce')\n",
    "\n",
    "    df.to_csv(\"full_employee_attendance_3.csv\", index=False)\n",
    "    print(f\"✅ Exported {len(df)} rows to 'full_employee_attendance.csv'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_full_attendance_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c40a18a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "BulkWriteError",
     "evalue": "batch op errors occurred, full error: {'writeErrors': [{'index': 0, 'code': 52, 'errmsg': \"_id fields may not contain '$'-prefixed fields: $oid is not valid for storage.\", 'op': {'_id': {'$oid': '31ae04022923927678dd0c3c'}, 'user': {'$oid': '681b05f9f095501f635801ca'}, 'date': {'$date': '2025-01-23T00:00:00Z'}, 'checkIn': {'time': {'$date': '2025-01-23T10:30:00Z'}, 'location': {'latitude': 28.600775, 'longitude': 77.161215}, 'timezone': 'Asia/Calcutta'}, 'checkOut': {'time': {'$date': '2025-01-23T20:07:48Z'}, 'location': {'latitude': 28.566447, 'longitude': 77.271916}, 'timezone': 'Asia/Calcutta'}, 'totalHours': 9.63, 'overworkingMinutes': 37.8, 'status': 'present', 'breaks': [{'_id': {'$oid': '7037c4ae326b7c30fd03327d'}, 'startTime': {'$date': '2025-01-23T13:12:00Z'}, 'endTime': {'$date': '2025-01-23T13:33:00Z'}, 'duration': 0.35, 'timezone': 'Asia/Calcutta'}, {'_id': {'$oid': '81d7b4b3db8e650bc50a40bb'}, 'startTime': {'$date': '2025-01-23T12:11:00Z'}, 'endTime': {'$date': '2025-01-23T12:36:48Z'}, 'duration': 0.43, 'timezone': 'Asia/Calcutta'}], 'createdAt': {'$date': '2025-05-08T06:41:15.530328Z'}, 'updatedAt': {'$date': '2025-05-08T06:41:15.530333Z'}, '__v': 1}}], 'writeConcernErrors': [], 'nInserted': 0, 'nUpserted': 0, 'nMatched': 0, 'nModified': 0, 'nRemoved': 0, 'upserted': []}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBulkWriteError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Insert into MongoDB\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m---> 26\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert_many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInserted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(result\u001b[38;5;241m.\u001b[39minserted_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m attendance records.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\.conda\\envs\\lida-demo\\lib\\site-packages\\pymongo\\_csot.py:119\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[0;32m    118\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\.conda\\envs\\lida-demo\\lib\\site-packages\\pymongo\\synchronous\\collection.py:975\u001b[0m, in \u001b[0;36mCollection.insert_many\u001b[1;34m(self, documents, ordered, bypass_document_validation, session, comment)\u001b[0m\n\u001b[0;32m    973\u001b[0m blk \u001b[38;5;241m=\u001b[39m _Bulk(\u001b[38;5;28mself\u001b[39m, ordered, bypass_document_validation, comment\u001b[38;5;241m=\u001b[39mcomment)\n\u001b[0;32m    974\u001b[0m blk\u001b[38;5;241m.\u001b[39mops \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(gen())\n\u001b[1;32m--> 975\u001b[0m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINSERT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m InsertManyResult(inserted_ids, write_concern\u001b[38;5;241m.\u001b[39macknowledged)\n",
      "File \u001b[1;32mc:\\Users\\admin\\.conda\\envs\\lida-demo\\lib\\site-packages\\pymongo\\synchronous\\bulk.py:751\u001b[0m, in \u001b[0;36m_Bulk.execute\u001b[1;34m(self, write_concern, session, operation)\u001b[0m\n\u001b[0;32m    749\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\.conda\\envs\\lida-demo\\lib\\site-packages\\pymongo\\synchronous\\bulk.py:614\u001b[0m, in \u001b[0;36m_Bulk.execute_command\u001b[1;34m(self, generator, write_concern, session, operation)\u001b[0m\n\u001b[0;32m    604\u001b[0m _ \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39m_retryable_write(\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_retryable,\n\u001b[0;32m    606\u001b[0m     retryable_bulk,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    610\u001b[0m     operation_id\u001b[38;5;241m=\u001b[39mop_id,\n\u001b[0;32m    611\u001b[0m )\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwriteErrors\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m full_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwriteConcernErrors\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 614\u001b[0m     \u001b[43m_raise_bulk_write_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m full_result\n",
      "File \u001b[1;32mc:\\Users\\admin\\.conda\\envs\\lida-demo\\lib\\site-packages\\pymongo\\bulk_shared.py:131\u001b[0m, in \u001b[0;36m_raise_bulk_write_error\u001b[1;34m(full_result)\u001b[0m\n\u001b[0;32m    125\u001b[0m         errmsg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    126\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis MongoDB deployment does not support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretryable writes. Please add retryWrites=false \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto your connection string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    129\u001b[0m         )\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OperationFailure(errmsg, code, full_result)\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m BulkWriteError(full_result)\n",
      "\u001b[1;31mBulkWriteError\u001b[0m: batch op errors occurred, full error: {'writeErrors': [{'index': 0, 'code': 52, 'errmsg': \"_id fields may not contain '$'-prefixed fields: $oid is not valid for storage.\", 'op': {'_id': {'$oid': '31ae04022923927678dd0c3c'}, 'user': {'$oid': '681b05f9f095501f635801ca'}, 'date': {'$date': '2025-01-23T00:00:00Z'}, 'checkIn': {'time': {'$date': '2025-01-23T10:30:00Z'}, 'location': {'latitude': 28.600775, 'longitude': 77.161215}, 'timezone': 'Asia/Calcutta'}, 'checkOut': {'time': {'$date': '2025-01-23T20:07:48Z'}, 'location': {'latitude': 28.566447, 'longitude': 77.271916}, 'timezone': 'Asia/Calcutta'}, 'totalHours': 9.63, 'overworkingMinutes': 37.8, 'status': 'present', 'breaks': [{'_id': {'$oid': '7037c4ae326b7c30fd03327d'}, 'startTime': {'$date': '2025-01-23T13:12:00Z'}, 'endTime': {'$date': '2025-01-23T13:33:00Z'}, 'duration': 0.35, 'timezone': 'Asia/Calcutta'}, {'_id': {'$oid': '81d7b4b3db8e650bc50a40bb'}, 'startTime': {'$date': '2025-01-23T12:11:00Z'}, 'endTime': {'$date': '2025-01-23T12:36:48Z'}, 'duration': 0.43, 'timezone': 'Asia/Calcutta'}], 'createdAt': {'$date': '2025-05-08T06:41:15.530328Z'}, 'updatedAt': {'$date': '2025-05-08T06:41:15.530333Z'}, '__v': 1}}], 'writeConcernErrors': [], 'nInserted': 0, 'nUpserted': 0, 'nMatched': 0, 'nModified': 0, 'nRemoved': 0, 'upserted': []}"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# MongoDB setup\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
    "DATABASE_NAME = \"ems\"\n",
    "COLLECTION_NAME = \"attendances\"\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DATABASE_NAME]\n",
    "collection = db[COLLECTION_NAME]\n",
    "\n",
    "# Load the JSON file\n",
    "json_file_path = \"sample_attendances_clean.json\"  # Make sure this path is correct\n",
    "with open(json_file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Insert into MongoDB\n",
    "if isinstance(data, list):\n",
    "    result = collection.insert_many(data)\n",
    "    print(f\"Inserted {len(result.inserted_ids)} attendance records.\")\n",
    "else:\n",
    "    print(\"Data format invalid. Expected a list of documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15163561",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8411f1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 100 documents into MongoDB.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bson import ObjectId\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "from dateutil import parser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# MongoDB connection\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[\"ems\"]\n",
    "collection = db[\"attendances\"]\n",
    "\n",
    "# Load and clean data\n",
    "def clean_extended_json(doc):\n",
    "    def convert_value(val):\n",
    "        if isinstance(val, dict):\n",
    "            if \"$oid\" in val:\n",
    "                return ObjectId(val[\"$oid\"])\n",
    "            elif \"$date\" in val:\n",
    "                return parser.parse(val[\"$date\"])\n",
    "            else:\n",
    "                return {k: convert_value(v) for k, v in val.items()}\n",
    "        elif isinstance(val, list):\n",
    "            return [convert_value(item) for item in val]\n",
    "        else:\n",
    "            return val\n",
    "\n",
    "    return {k: convert_value(v) for k, v in doc.items()}\n",
    "\n",
    "with open(\"final_attendances_clean.json\", \"r\") as file:\n",
    "    raw_data = json.load(file)\n",
    "\n",
    "cleaned_data = [clean_extended_json(doc) for doc in raw_data]\n",
    "\n",
    "# Insert into MongoDB\n",
    "if cleaned_data:\n",
    "    result = collection.insert_many(cleaned_data)\n",
    "    print(f\"Inserted {len(result.inserted_ids)} documents into MongoDB.\")\n",
    "else:\n",
    "    print(\"No data found to insert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20d241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lida-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
